{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kennethasmith/bias-breaker-ai-agent?scriptVersionId=287085991\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\n\n# --- 1. Custom Tool Function (Function remains the same) ---\ndef calculate_fairness_metrics(\n    df_json: str, \n    protected_attribute: str,\n    predictions_column: str,\n    favorable_outcome: Any,\n    unprivileged_group_value: Any,\n) -> str:\n    \"\"\"\n    Calculates key fairness metrics (e.g., Disparate Impact Ratio) and \n    runs a simulated feature contribution analysis.\n    \n    Args:\n        df_json: JSON string of the dataset and predictions.\n        protected_attribute: The column name for the protected group (e.g., 'gender').\n        predictions_column: The column name containing the model's predictions.\n        favorable_outcome: The value representing the positive outcome (e.g., 1 or 'Approved').\n        unprivileged_group_value: The value representing the unprivileged group (e.g., 'Female').\n        \n    Returns:\n        A comprehensive metrics and feature analysis report as a string.\n    \"\"\"\n    try:\n        # Load the data from the JSON string passed via the session\n        df = pd.read_json(df_json)\n        \n        # --- Bias Metric Calculation: Disparate Impact Ratio (DIR) ---\n        unprivileged_df = df[df[protected_attribute] == unprivileged_group_value]\n        privileged_df = df[df[protected_attribute] != unprivileged_group_value]\n\n        rate_unprivileged = (unprivileged_df[predictions_column] == favorable_outcome).mean()\n        rate_privileged = (privileged_df[predictions_column] == favorable_outcome).mean()\n        \n        dir_value = rate_unprivileged / rate_privileged if rate_privileged != 0 else float('inf')\n\n        # --- Proxy Bias/Feature Contribution Detection (Simulated for LLM analysis) ---\n        feature_finding = \"\"\n        if 'zip_code' in df.columns and dir_value < 0.8:\n            feature_finding = \"The 'zip_code' feature appears to be a strong **proxy for the protected attribute** (likely demographic data), contributing significantly to the disparate impact. It should be investigated for removal or masking.\"\n        elif dir_value < 0.95:\n             feature_finding = \"Feature analysis suggests direct model dependence on the protected attribute itself. Re-weighting or pre-processing techniques are needed.\"\n        else:\n            feature_finding = \"No clear proxy features detected, but the attribute itself is causing the disparity.\"\n        \n        # --- Format Report ---\n        report = f\"\"\"\nFAIRNESS ANALYSIS REPORT for Attribute: {protected_attribute}\n==================================================\n\nPrimary Metric: Disparate Impact Ratio (DIR)\n- Value: {dir_value:.4f}\n- Violation Threshold: DIR below 0.8 or above 1.25 indicates significant bias.\n- Bias Status: {'VIOLATION DETECTED' if dir_value < 0.8 or dir_value > 1.25 else 'PASS'}\n\nProtected Group ({unprivileged_group_value}) Success Rate: {rate_unprivileged:.4f}\nReference Group Success Rate: {rate_privileged:.4f}\n\nFEATURE CONTRIBUTION FINDING:\n- Identified Contribution: {feature_finding}\n\nSummary: The model exhibits **{('significant bias' if dir_value < 0.8 or dir_value > 1.25 else 'minor disparity')}** against the unprivileged group.\n\"\"\"\n        return report\n        \n    except Exception as e:\n        return f\"ERROR: Failed to run bias calculation: {e}\"\n\n# --- 2. Create the ADK Function Tool (CORRECTED) ---\nfairness_tool = FunctionTool(calculate_fairness_metrics) # Pass the function directly!\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:22:43.855132Z","iopub.execute_input":"2025-12-18T16:22:43.855586Z","iopub.status.idle":"2025-12-18T16:22:47.459028Z","shell.execute_reply.started":"2025-12-18T16:22:43.855548Z","shell.execute_reply":"2025-12-18T16:22:47.457181Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2395030948.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprotected_attribute\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpredictions_column\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfavorable_outcome\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0munprivileged_group_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m ) -> str:\n","\u001b[0;31mNameError\u001b[0m: name 'Any' is not defined"],"ename":"NameError","evalue":"name 'Any' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"#Bring in API Key\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"âœ… Gemini API key configured successfully.\")\nexcept Exception as e:\n    print(f\"ðŸ”’ Authentication Error: {e}\")\n    print(\"Please add 'GOOGLE_API_KEY' to your Kaggle secrets.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:22:47.460221Z","iopub.status.idle":"2025-12-18T16:22:47.46063Z","shell.execute_reply.started":"2025-12-18T16:22:47.46046Z","shell.execute_reply":"2025-12-18T16:22:47.460478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Import neccessary packages\nimport numpy as np\nimport json\nimport asyncio\nfrom typing import Any, Dict, List\nfrom google.genai import types\nfrom google.adk.agents import LlmAgent, SequentialAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner  # Key change based on reference\nfrom google.adk.sessions import InMemorySessionService # Key change based on reference\nfrom google.adk.tools import FunctionTool\n\n# Retry Config\nretry_config = types.HttpRetryOptions(\n    attempts=5,\n    exp_base=7,\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],\n)\n\n# Models\nGEMINI_FLASH = Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config)\nGEMINI_PRO = Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config)\n\n# Defining Memory Bank & Tools \n\n# We use a global dictionary to mimic the \"Memory Bank\" pattern\nSESSION_MEMORY: Dict[str, Any] = {}\n\ndef save_report_tool(key: str, content: str) -> dict:\n    \"\"\"Tool to save reports/data to the global memory bank.\"\"\"\n    SESSION_MEMORY[key] = content\n    return {\"status\": \"success\", \"saved_key\": key}\n\ndef get_report_tool(key: str) -> dict:\n    \"\"\"Tool to retrieve reports/data from the global memory bank.\"\"\"\n    data = SESSION_MEMORY.get(key)\n    if data is None:\n        return {\"status\": \"error\", \"message\": \"Key not found\"}\n    return {\"status\": \"success\", \"content\": data}\n\n# Custom Tool for Fairness Calculation\ndef calculate_fairness_metrics(\n    df_json: str, \n    protected_attribute: str,\n    predictions_column: str,\n    favorable_outcome: int,\n    unprivileged_group_value: str,\n) -> str:\n    \"\"\"Calculates Disparate Impact Ratio from JSON data.\"\"\"\n    try:\n        df = pd.read_json(df_json)\n        \n        # Calculate metrics\n        unprivileged = df[df[protected_attribute] == unprivileged_group_value]\n        privileged = df[df[protected_attribute] != unprivileged_group_value]\n        \n        rate_unpriv = (unprivileged[predictions_column] == favorable_outcome).mean()\n        rate_priv = (privileged[predictions_column] == favorable_outcome).mean()\n        \n        if rate_priv == 0:\n            dir_val = 0.0\n        else:\n            dir_val = rate_unpriv / rate_priv\n            \n        return json.dumps({\n            \"metric\": \"Disparate Impact Ratio\",\n            \"value\": dir_val,\n            \"status\": \"BIASED\" if (dir_val < 0.8 or dir_val > 1.25) else \"FAIR\"\n        })\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# --- 4. Agent Definitions (Using LlmAgent like reference) ---\n\n# Agent 1: DataPrepAgent\n# Stores the raw data into the shared memory\ndata_prep_agent = LlmAgent(\n    name=\"DataPrepAgent\",\n    model=GEMINI_FLASH,\n    instruction=\"\"\"\n    You are a Data Ingestion Specialist.\n    1. Accept the raw dataset JSON provided in the prompt.\n    2. Call `save_report_tool` to save it under the key 'raw_data'.\n    3. Confirm the data is saved and ready for analysis.\n    \"\"\",\n    tools=[save_report_tool]\n)\n\n# Agent 2: MetricsAgent\n# Retrieves data, runs calculation, saves result\nmetrics_agent = LlmAgent(\n    name=\"MetricsAgent\",\n    model=GEMINI_PRO,\n    instruction=\"\"\"\n    You are an AI Metric Specialist.\n    1. Retrieve 'raw_data' using `get_report_tool`.\n    2. Use the `calculate_fairness_metrics` tool. \n       (Params: protected_attribute='gender', predictions_column='pred', favorable_outcome=1, unprivileged_group_value='Female')\n    3. Call `save_report_tool` to save the tool's output JSON under the key 'metrics_result'.\n    \"\"\",\n    tools=[get_report_tool, save_report_tool, calculate_fairness_metrics]\n)\n\n# Agent 3: DiagnosisAgent\n# Retrieves metrics, analyzes them\ndiagnosis_agent = LlmAgent(\n    name=\"DiagnosisAgent\",\n    model=GEMINI_PRO,\n    instruction=\"\"\"\n    You are a Fairness Diagnostician.\n    1. Retrieve 'metrics_result' using `get_report_tool`.\n    2. Analyze the Disparate Impact Ratio.\n    3. If BIASED, explain WHY (e.g., historical bias in training data).\n    4. Call `save_report_tool` to save your diagnosis text under 'diagnosis'.\n    \"\"\",\n    tools=[get_report_tool, save_report_tool]\n)\n\n# Agent 4: ScorecardAgent (Final Reporter)\nscorecard_agent = LlmAgent(\n    name=\"ScorecardAgent\",\n    model=GEMINI_FLASH,\n    instruction=\"\"\"\n    You are the Final Report Generator.\n    1. Retrieve 'metrics_result' and 'diagnosis' using `get_report_tool`.\n    2. Generate a final Markdown 'Fairness Scorecard'.\n    3. Output ONLY the Markdown report.\n    \"\"\",\n    tools=[get_report_tool]\n)\n\n# --- 5. Orchestration (SequentialAgent) ---\n\nbias_breaker_pipeline = SequentialAgent(\n    name=\"BiasBreakerPipeline\",\n    sub_agents=[\n        data_prep_agent,\n        metrics_agent,\n        diagnosis_agent,\n        scorecard_agent\n    ],\n)\n\n# --- 6. Execution (Using InMemoryRunner) ---\n\n# Create Runner \nrunner = InMemoryRunner(bias_breaker_pipeline)\n\n# Prepare Data\ntest_df = pd.DataFrame({\n    'gender': ['Female', 'Male', 'Female', 'Male'] * 25,\n    'pred': [0, 1, 0, 1] * 25 \n})\ndataset_json = test_df.to_json(orient='records')\n\n# Initial Prompt containing the data\nprompt = f\"\"\"\nSTART ANALYSIS.\nHere is the raw dataset JSON:\n{dataset_json}\n\"\"\"\n\nprint(\"ðŸš€ Starting Bias Breaker Pipeline (Reference Architecture)...\")\n\n# Execute\n# Note: In a notebook, you usually await this:\nresponse = await runner.run_debug(prompt)\n\n# Print Final Output\nprint(\"\\n\" + \"=\"*50)\nprint(\"âœ… FINAL AGENT OUTPUT\")\nprint(\"=\"*50)\n\n# Iterate through turns to find the final scorecard (similar to reference cell 10)\nfor turn in response:\n    if getattr(turn, \"source\", \"\") == \"ScorecardAgent\":\n        if hasattr(turn, \"content\") and turn.content and turn.content.text:\n            print(turn.content.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T16:22:47.463457Z","iopub.status.idle":"2025-12-18T16:22:47.463939Z","shell.execute_reply.started":"2025-12-18T16:22:47.463715Z","shell.execute_reply":"2025-12-18T16:22:47.463736Z"}},"outputs":[],"execution_count":null}]}