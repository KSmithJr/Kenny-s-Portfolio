{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kennethasmith/bias-breaker-ai-agent?scriptVersionId=287824078\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#Import neccessary packages\nimport pandas as pd\nimport numpy as np\nimport json\nimport asyncio\nfrom typing import Any, Dict, List\nfrom google.genai import types\nfrom google.adk.agents import LlmAgent, SequentialAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner  # Key change based on reference\nfrom google.adk.sessions import InMemorySessionService # Key change based on reference\nfrom google.adk.tools import FunctionTool\n\n# --- 1. Custom Tool Function (Function remains the same) ---\ndef calculate_fairness_metrics(\n    df_json: str, \n    protected_attribute: str,\n    predictions_column: str,\n    favorable_outcome: Any,\n    unprivileged_group_value: Any,\n) -> str:\n    \"\"\"\n    Calculates key fairness metrics (e.g., Disparate Impact Ratio) and \n    runs a simulated feature contribution analysis.\n    \n    Args:\n        df_json: JSON string of the dataset and predictions.\n        protected_attribute: The column name for the protected group (e.g., 'gender').\n        predictions_column: The column name containing the model's predictions.\n        favorable_outcome: The value representing the positive outcome (e.g., 1 or 'Approved').\n        unprivileged_group_value: The value representing the unprivileged group (e.g., 'Female').\n        \n    Returns:\n        A comprehensive metrics and feature analysis report as a string.\n    \"\"\"\n    try:\n        # Load the data from the JSON string passed via the session\n        df = pd.read_json(df_json)\n        \n        # --- Bias Metric Calculation: Disparate Impact Ratio (DIR) ---\n        unprivileged_df = df[df[protected_attribute] == unprivileged_group_value]\n        privileged_df = df[df[protected_attribute] != unprivileged_group_value]\n\n        rate_unprivileged = (unprivileged_df[predictions_column] == favorable_outcome).mean()\n        rate_privileged = (privileged_df[predictions_column] == favorable_outcome).mean()\n        \n        dir_value = rate_unprivileged / rate_privileged if rate_privileged != 0 else float('inf')\n\n        # --- Proxy Bias/Feature Contribution Detection (Simulated for LLM analysis) ---\n        feature_finding = \"\"\n        if 'zip_code' in df.columns and dir_value < 0.8:\n            feature_finding = \"The 'zip_code' feature appears to be a strong **proxy for the protected attribute** (likely demographic data), contributing significantly to the disparate impact. It should be investigated for removal or masking.\"\n        elif dir_value < 0.95:\n             feature_finding = \"Feature analysis suggests direct model dependence on the protected attribute itself. Re-weighting or pre-processing techniques are needed.\"\n        else:\n            feature_finding = \"No clear proxy features detected, but the attribute itself is causing the disparity.\"\n        \n        # --- Format Report ---\n        report = f\"\"\"\nFAIRNESS ANALYSIS REPORT for Attribute: {protected_attribute}\n==================================================\n\nPrimary Metric: Disparate Impact Ratio (DIR)\n- Value: {dir_value:.4f}\n- Violation Threshold: DIR below 0.8 or above 1.25 indicates significant bias.\n- Bias Status: {'VIOLATION DETECTED' if dir_value < 0.8 or dir_value > 1.25 else 'PASS'}\n\nProtected Group ({unprivileged_group_value}) Success Rate: {rate_unprivileged:.4f}\nReference Group Success Rate: {rate_privileged:.4f}\n\nFEATURE CONTRIBUTION FINDING:\n- Identified Contribution: {feature_finding}\n\nSummary: The model exhibits **{('significant bias' if dir_value < 0.8 or dir_value > 1.25 else 'minor disparity')}** against the unprivileged group.\n\"\"\"\n        return report\n        \n    except Exception as e:\n        return f\"ERROR: Failed to run bias calculation: {e}\"\n\n# --- 2. Create the ADK Function Tool (CORRECTED) ---\nfairness_tool = FunctionTool(calculate_fairness_metrics) # Pass the function directly!\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T17:07:53.827766Z","iopub.execute_input":"2025-12-18T17:07:53.828064Z","iopub.status.idle":"2025-12-18T17:07:53.841306Z","shell.execute_reply.started":"2025-12-18T17:07:53.828037Z","shell.execute_reply":"2025-12-18T17:07:53.839682Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Bring in API Key\nfrom kaggle_secrets import UserSecretsClient\nimport os\n\n# This line fixes the \"name 'os' is not defined\" error\nuser_secrets = UserSecretsClient()\nos.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\nprint(\"âœ… API Key successfully loaded into environment!\")\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"âœ… Gemini API key configured successfully.\")\nexcept Exception as e:\n    print(f\"ðŸ”’ Authentication Error: {e}\")\n    print(\"Please add 'GOOGLE_API_KEY' to your Kaggle secrets.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T17:07:12.348741Z","iopub.execute_input":"2025-12-18T17:07:12.349233Z","iopub.status.idle":"2025-12-18T17:07:12.439485Z","shell.execute_reply.started":"2025-12-18T17:07:12.349205Z","shell.execute_reply":"2025-12-18T17:07:12.438454Z"}},"outputs":[{"name":"stdout","text":"âœ… API Key successfully loaded into environment!\nâœ… Gemini API key configured successfully.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#Import neccessary packages\nimport numpy as np\nimport json\nimport asyncio\nfrom typing import Any, Dict, List\nfrom google.genai import types\nfrom google.adk.agents import LlmAgent, SequentialAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner  # Key change based on reference\nfrom google.adk.sessions import InMemorySessionService # Key change based on reference\nfrom google.adk.tools import FunctionTool\n\n# Retry Config\nretry_config = types.HttpRetryOptions(\n    attempts=5,\n    exp_base=7,\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],\n)\n\n# Models\nGEMINI_FLASH = Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config)\nGEMINI_PRO = Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config)\n\n# Defining Memory Bank & Tools \n\n# We use a global dictionary to mimic the \"Memory Bank\" pattern\nSESSION_MEMORY: Dict[str, Any] = {}\n\ndef save_report_tool(key: str, content: str) -> dict:\n    \"\"\"Tool to save reports/data to the global memory bank.\"\"\"\n    SESSION_MEMORY[key] = content\n    return {\"status\": \"success\", \"saved_key\": key}\n\ndef get_report_tool(key: str) -> dict:\n    \"\"\"Tool to retrieve reports/data from the global memory bank.\"\"\"\n    data = SESSION_MEMORY.get(key)\n    if data is None:\n        return {\"status\": \"error\", \"message\": \"Key not found\"}\n    return {\"status\": \"success\", \"content\": data}\n\n# Custom Tool for Fairness Calculation\ndef calculate_fairness_metrics(\n    df_json: str, \n    protected_attribute: str,\n    predictions_column: str,\n    favorable_outcome: int,\n    unprivileged_group_value: str,\n) -> str:\n    \"\"\"Calculates Disparate Impact Ratio from JSON data.\"\"\"\n    try:\n        df = pd.read_json(df_json)\n        \n        # Calculate metrics\n        unprivileged = df[df[protected_attribute] == unprivileged_group_value]\n        privileged = df[df[protected_attribute] != unprivileged_group_value]\n        \n        rate_unpriv = (unprivileged[predictions_column] == favorable_outcome).mean()\n        rate_priv = (privileged[predictions_column] == favorable_outcome).mean()\n        \n        if rate_priv == 0:\n            dir_val = 0.0\n        else:\n            dir_val = rate_unpriv / rate_priv\n            \n        return json.dumps({\n            \"metric\": \"Disparate Impact Ratio\",\n            \"value\": dir_val,\n            \"status\": \"BIASED\" if (dir_val < 0.8 or dir_val > 1.25) else \"FAIR\"\n        })\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# --- 4. Agent Definitions (Using LlmAgent like reference) ---\n\n# Agent 1: DataPrepAgent\n# Stores the raw data into the shared memory\ndata_prep_agent = LlmAgent(\n    name=\"DataPrepAgent\",\n    model=GEMINI_FLASH,\n    instruction=\"\"\"\n    You are a Data Ingestion Specialist.\n    1. Accept the raw dataset JSON provided in the prompt.\n    2. Call `save_report_tool` to save it under the key 'raw_data'.\n    3. Confirm the data is saved and ready for analysis.\n    \"\"\",\n    tools=[save_report_tool]\n)\n\n# Agent 2: MetricsAgent\n# Retrieves data, runs calculation, saves result\nmetrics_agent = LlmAgent(\n    name=\"MetricsAgent\",\n    model=GEMINI_PRO,\n    instruction=\"\"\"\n    You are an AI Metric Specialist.\n    1. Retrieve 'raw_data' using `get_report_tool`.\n    2. Use the `calculate_fairness_metrics` tool. \n       (Params: protected_attribute='gender', predictions_column='pred', favorable_outcome=1, unprivileged_group_value='Female')\n    3. Call `save_report_tool` to save the tool's output JSON under the key 'metrics_result'.\n    \"\"\",\n    tools=[get_report_tool, save_report_tool, calculate_fairness_metrics]\n)\n\n# Agent 3: DiagnosisAgent\n# Retrieves metrics, analyzes them\ndiagnosis_agent = LlmAgent(\n    name=\"DiagnosisAgent\",\n    model=GEMINI_PRO,\n    instruction=\"\"\"\n    You are a Fairness Diagnostician.\n    1. Retrieve 'metrics_result' using `get_report_tool`.\n    2. Analyze the Disparate Impact Ratio.\n    3. If BIASED, explain WHY (e.g., historical bias in training data).\n    4. Call `save_report_tool` to save your diagnosis text under 'diagnosis'.\n    \"\"\",\n    tools=[get_report_tool, save_report_tool]\n)\n\n# Agent 4: ScorecardAgent (Final Reporter)\nscorecard_agent = LlmAgent(\n    name=\"ScorecardAgent\",\n    model=GEMINI_FLASH,\n    instruction=\"\"\"\n    You are the Final Report Generator.\n    1. Retrieve 'metrics_result' and 'diagnosis' using `get_report_tool`.\n    2. Generate a final Markdown 'Fairness Scorecard'.\n    3. Output ONLY the Markdown report.\n    \"\"\",\n    tools=[get_report_tool]\n)\n\n# --- 5. Orchestration (SequentialAgent) ---\n\nbias_breaker_pipeline = SequentialAgent(\n    name=\"BiasBreakerPipeline\",\n    sub_agents=[\n        data_prep_agent,\n        metrics_agent,\n        diagnosis_agent,\n        scorecard_agent\n    ],\n)\n\n# --- 6. Execution (Using InMemoryRunner) ---\n\n# Create Runner \nrunner = InMemoryRunner(bias_breaker_pipeline)\n\n# Prepare Data\ntest_df = pd.DataFrame({\n    'gender': ['Female', 'Male', 'Female', 'Male'] * 25,\n    'pred': [0, 1, 0, 1] * 25 \n})\ndataset_json = test_df.to_json(orient='records')\n\n# Initial Prompt containing the data\nprompt = f\"\"\"\nSTART ANALYSIS.\nHere is the raw dataset JSON:\n{dataset_json}\n\"\"\"\n\nprint(\"ðŸš€ Starting Bias Breaker Pipeline (Reference Architecture)...\")\n\n# Execute\n# Note: In a notebook, you usually await this:\nresponse = await runner.run_debug(prompt)\n\n# Print Final Output\nprint(\"\\n\" + \"=\"*50)\nprint(\"âœ… FINAL AGENT OUTPUT\")\nprint(\"=\"*50)\n\n# Iterate through turns to find the final scorecard (similar to reference cell 10)\nfor turn in response:\n    if getattr(turn, \"source\", \"\") == \"ScorecardAgent\":\n        if hasattr(turn, \"content\") and turn.content and turn.content.text:\n            print(turn.content.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T17:08:02.90026Z","iopub.execute_input":"2025-12-18T17:08:02.900978Z","iopub.status.idle":"2025-12-18T17:08:18.253137Z","shell.execute_reply.started":"2025-12-18T17:08:02.900947Z","shell.execute_reply":"2025-12-18T17:08:18.251962Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting Bias Breaker Pipeline (Reference Architecture)...\n\n ### Created new session: debug_session_id\n\nUser > \nSTART ANALYSIS.\nHere is the raw dataset JSON:\n[{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1},{\"gender\":\"Female\",\"pred\":0},{\"gender\":\"Male\",\"pred\":1}]\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"DataPrepAgent > The raw dataset has been successfully saved and is ready for analysis.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\nWARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n/tmp/ipykernel_47/3797910165.py:52: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_json(df_json)\nWARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"MetricsAgent > The fairness metrics have been calculated and the report has been saved. The Disparate Impact Ratio is 0.0, which indicates that the model's predictions are BIASED against the unprivileged group ('Female').\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"DiagnosisAgent > The model's predictions are BIASED against the 'Female' group, as indicated by a Disparate Impact Ratio of 0.0. This suggests that the model is disproportionately unfavorable towards females in its predictions. This bias could stem from historical biases present in the training data, where past inequalities may have led to certain patterns that the model has learned and perpetuated.\n\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"ScorecardAgent > Hi, I'm the Final Report Generator. I will now generate the Fairness Scorecard based on the metrics and diagnosis.\n\n\n==================================================\nâœ… FINAL AGENT OUTPUT\n==================================================\n","output_type":"stream"}],"execution_count":12}]}